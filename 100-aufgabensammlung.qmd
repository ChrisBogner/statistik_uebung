# Aufgabensammlung

## Erste Schritte
```{r}
#| echo: false
#| warning: false
#| error: false
library(tidyverse)
library(kableExtra)
```

### Morphometrische Messungen an Vögeln
In einer Studie wurden 1100 Spitzschwanzammer (*Ammodramus caudacutus*) vermessen. Wir nutzen einen Teil des Datensatzes [@Zuur2009]. Die gemessenen Variablen sind Fluegel, Fuss (Tarsus), Kopf, Gewicht. Leider git die Datenquelle die Messeinheiten nicht an. Die Daten sind wie folgt:


```{r}
#| echo: false
Fluegel <- c(59, 55, 53.5, 55, 52.5, 57.5, 53, 55)
Fuss <- c(22.3, 19.7, 20.8, 20.3, 20.8, 21.5, 20.6, 21.5)
Kopf <- c(31.2, 30.4, 30.6, 30.3, 30.3, 30.8, 32.5, NA)
Gewicht <- c(9.5, 13.8, 14.8, 15.2, 15.5, 15.6, 15.6, 15.7)

birds <- tibble(Fluegel, Fuss, Kopf, Gewicht)
```

```{r}
#| echo: false
kable(birds) %>% 
  kable_styling(bootstrap_options = c("striped", "condensed"), 
                latex_options = c("striped", "hold_position"), full_width = FALSE) %>%
  column_spec(1:4, width = "10em")
```
1. Erstellen Sie jede Variable einzeln mithilfe der Funktion `c()`.
2. Wie viele Vögel sind in der Tabelle zu finden. Nutzen Sie dazu die Funktion `length()`. Sehen Sie in der Hilfe nach, wie man diese benutzt.
3. Führen Sie alle Variablen zu einem einzelnen Datenobjekt, einem `tibble` zusammen mithilfe der Funktion `tibble()` aus dem R-Paket `tibble`.


## Einführung in die Darstellung von Daten

### Pinguine

1. Laden Sie die Bibliotheken `tidyverse` und `palmerpenguins` mithilfe der Funktion `library()`.
2. Laden Sie den Datensatz `penguins` mithilfe der Funktion `data()`.
2. Sehen Sie sich den Datensatz an.
3. Plotten Sie ein Streudiagramm der Variablen Flossenlänge `flipper_length_mm` auf der $x$-Achse und der Variablen Körpergewicht `body_mass_g` auf der $y$-Achse.
3. Beschriften Sie die Grafik sinnvoll.
4. Färben Sie die Punkte je nach Art unterschiedlich ein mithilfe der Variablen `species`.

Sie sollten die gleiche (bis auf die Farbauswahl) Grafik erhalten, wie in der Vorlesung `r emo::ji('nerd')`.

## Daten in R einlesen und aus R speichern 

### Politbarometer 2021: Einlesen von Fremdformaten {#sec-aufgabe-politbarometer-einlesen}
Es gibt viele verschiedene Statistikpakete (z. B. SAS, SPSS, Stata), die mit grafischen Oberflächen arbeiten. Da die Analysen darin nicht *reproducible* sind (weil mit der Maus zusammengeklickt), empfehlen wir diese nicht. Dennoch gibt es manchmal interessante Datensätze, die in den Formaten dieser Statistikpakete vorliegen. ACHTUNG: Diese Aufgabe ist anspruchsvoll!

In dieser Übung lernen Sie das Paket [haven](https://haven.tidyverse.org/) kennen, dass solche Formate einlesen kann. Haven ist Teil von `tidyverse`, muss aber extra installiert und geladen werden.

1. Laden Sie die Bibliotheken `tidyverse` und `haven`.

Wir beschäftigen uns mit dem Datensatz "Politbarometer 2021". Die Politbarometer kennen Sie bestimmt aus dem ZDF. Das sind Telefonumfragen, die seit 1977 etwa monatlich von der Forschungsgruppe Wahlen für das ZDF durchgeführt werden. Wir sehen uns die Daten aus dem Jahr 2021 an. Sie sind für Lehre und Forschung frei. Sie müssen Sie jedoch selbst herunterladen, die Nutzungsbedingungen lesen und ihnen zustimmen. Die Daten gibt es hier: http://dx.doi.org/10.4232/1.13909.

2. Laden Sie unter "Downloads" (rechts oben) den Datensatz "ZA7856_v1-0-0.dta.zip Stata (Dataset) 1.9 MB" herunter. Dafür werden Sie sich einmalig (und kostenlos) anmelden müssen.

Das ist ein komprimierter Datensatz des Statistikpakets Stata. Speichern Sie den Datensatz in Ihrem "Daten"-Ordner und entpacken Sie ihn dort. Es wird ein Ordner namens ZA7856_v1-0-0.dta erstellt, in dem Sie die Datei "ZA7856_v1-0-0.dta" finden. Das ist der eigentliche Datensatz.

3. Datensatz einlesen mit der Funktion read_dta(). Passen Sie den Pfad zur Datei an, da ich für die Übung eine andere Verzeichnisstruktur habe!
```{r}
#| eval: false
gesis <- read_dta('Daten/ZA7856_v1-0-0.dta/ZA7856_v1-0-0.dta')
```


4. Wie viele Beobachtungen und Variablen enthält der Datensatz?


5. Die Variablennamen sind nichtssagend. Um den Datensatz zu verstehen, laden Sie auf der GESIS-Seite das Codebook herunter (rechts oben bei Downloads). Die Variablennamen sind in der "Tabelle 1: Variablenkorrespondenzliste Politbarometer 2021" gelistet.

6. Wir werden gemeinsam die Variablen richtig umbenennen und die kategorialen Variablen zu Faktoren ändern. Gehen Sie durch den Code Zeile für Zeile durch und erklären Sie, was dieser macht.

```{r}
#| eval: false
gesis_short <- gesis %>% 
  rename(Befragtennummer = V2,
         Erhebungsmonat = V4,
         Erhebungswoche = V5,
         Bundesland = V6,
         Erhebungsgebiet = V7,
         Einwohner = V8,
         Polit_interesse = V124) %>%
  mutate(Erhebungsmonat = as_factor(Erhebungsmonat),
         Erhebungswoche = as_factor(Erhebungswoche),
         Bundesland = as_factor(Bundesland),
         Erhebungsgebiet = as_factor(Erhebungsgebiet),
         Einwohner = as_factor(Einwohner),
         Polit_interesse = as_factor(Polit_interesse)
         ) %>% 
  select(Befragtennummer,
         Erhebungsmonat,
         Erhebungswoche,
         Bundesland,
         Erhebungsgebiet,
         Einwohner,
         Polit_interesse)
```

7. Wie hat sich der Typ der kategorialen Variablen im Datensatz `gesis_short` gegenüber dem ursprünglichen Datensatz `gesis` verändert?

8. Speichern Sie den neuen Datensatz `gesis_short` mit `write_delim()` ab.

## Exploration von kategorialen Daten

### Politbarometer 2021: Das Interesse für Politik {#sec-aufgabe-politbarometer-exploration-kategorial}
Wir analysieren den Datensatz, den Sie in der vorherigen Übung geladen und vorbereitet haben.

1. Laden Sie nun den kurzen Datensatz `gesis_short` mit der passenden Bibliothek ein. Sie müssen vorher natürlich diese Bibliothek mit `library()` laden.

```{r}
#| echo: false
#| warning: false
#| message: false
gesis_short <- read_delim('Daten/gesis_short.csv', delim = ';') %>% 
  mutate(Bundesland = as_factor(Bundesland)) %>%
  mutate(Polit_interesse = factor(Polit_interesse, levels = c('Sehr stark', 'stark', 'etwas', 'kaum', 'gar nicht', 'KA')))
```


2. Untersuchen Sie den Datensatz nach dem Laden. Wie sind die kategorialen Variablen kodiert (chr odr fct)? Warum? Sehen Sie in der Hilfe von `read_delim` nach.

3. Wir müssen nach dem Einlesen die kategorialen Variablen erneut in Faktoren umwandeln. Diese Information geht durch das Speichern mit `write_delim()` und das erneute Einlesen mit `read_delim()` verloren. Wandeln Sie die Variable `Bundesland` in einen Faktor um. Wenn Sie mit der Funktion `as_fcator()` arbeiten, ist die Reihenfolge der Merkmalsausprägungen (der unterschiedlichen Werte einer kategorialen Variablen) standardmäßig so, wie diese im Datensatz erscheinen. Das ist für die Bundesländer ausreichend.

4. Wie viele Personen wurden pro Bundesland im Politbarometer im Jahr 2021 befragt?


5. Wir wollen nun wissen, wie das Politikinteresse in den Bundesländern ausgeprägt ist. Dafür sehen wir uns die Antworten auf die Frage "Wie stark interessieren Sie sich für Politik, ...". Die Antworten sind in der Variablen `Polit_Interesse` enthalten. Wie haben die Befragten abgestimmt?

6. Die Reihenfolge der Merkmalsausprägungen ist unlogisch. Das müssen wir ändern. Bei dieser Variablen gibt es eine logische Reihenfolge: Sehr stark, stark, etwas, kaum, gar nicht, KA. Letzteres steht für keine Angabe. Nutzen Sie den folgenden Code, um die Variable `Polit_interesse` in einen Faktor mit richtiger Reihenfolge der Merkmalsausprägungen umzuwandeln.
```{r}
#| eval: false
gesis_short <- gesis_short %>% 
  mutate(gesis_short <- gesis_short %>% 
  mutate(Polit_interesse = factor(Polit_interesse, levels = c('Sehr stark', 'stark', 'etwas', 'kaum', 'gar nicht', 'KA'))))
```

Wiederholen Sie nun die Aufgabe 5.

7. Vergleichen Sie die Antworten zwischen den Bundesländern. Ist das Interesse der Bürger ähnlich? Warum ist das schwer zu beantworten?

8. Wir pirschen uns an die relativen Häufigkeiten heran. Was macht der nachfolgende Code. Sehen Sie gegebenenfalls in der Hilfe nach.
```{r}
#| eval: false
gesis_short %>% 
  count(Bundesland, Polit_interesse) %>% 
  pivot_wider(names_from = Bundesland, values_from = n)
```

Der nächste Schritt ist es, die relativen Häufigkeiten (Anteile) für jedes Bundesland auszurechnen, um die obige Frage zu beantworten. Erklären Sie, was der nachfolgende Code macht:
```{r}
#| eval: false
gesis_short %>% 
  count(Bundesland, Polit_interesse) %>% 
  group_by(Bundesland) %>%
  mutate(Anteil = n / sum(n)) %>% 
  select(-n) %>% 
  pivot_wider(names_from = Bundesland, values_from = Anteil)
```

Zurück zu unserer Frage: Ist das Interesse der Bürger in allen Bundesländern ähnlich?

9. Beantworten Sie die Frage jetzt auch grafisch, indem Sie ein Balkendiagramm plotten. Es soll so aussehen:
```{r}
#| echo: false
ggplot(data = gesis_short, mapping = aes(y = Bundesland, fill = Polit_interesse)) +
  geom_bar(position = position_fill(reverse = TRUE)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  labs(x = 'Anteile', y = 'Bundesland', fill = 'Politisches Interesse')
```

Dafür können Sie folgende Code-Fragmente ergänzen:
```{r}
#| eval: false
ggplot(data = ___, mapping = aes(y = ___, fill = ___)) +
  geom_bar(position = position_fill(reverse = TRUE)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  labs(___)
```

Was macht `geom_bar(position = position_fill(reverse = TRUE))`?

## Exploration von numerischen Daten

### Umweltdaten entlang der dänischen Küste

Die Datei "Temperatur.csv" aus @Zuur2009a enthält Messungen von Temperatur, Salinität und Chlorophyll a an 31 Orten entlang der dänischen Küste. Der Datensatz kann [hier](https://highstat.com/index.php/a-beginner-s-guide-to-r) heruntergeladen werden. Sie bekommen ihn aber bereits über ILIAS gestellt. Die Daten stammen vom dänischen Institut RIKZ (Monitoringprogramm MWTL: Monitoring Waterstaatkundige Toestand des Lands). Die Messungen wurden zwischen 1990 und 2005 durchgeführt, mit einer Häufigkeit von 0--4 mal pro Monat je nach Jahreszeit.

1. Lesen Sie den Datensatz "Temperatur.csv" (auf ILIAS) ein.
1. Konvertieren Sie die Spalte Date in ein richtiges Datumsformat und plotten Sie die Temperaturen pro Station (`facet_wrap()`) als Zeitreihen.
1. Berechnen Sie die Anzahl der Messwerte, Monatsmittelwerte der Temperatur für alle Stationen, sowie die Standardabweichungen. Tipp: innerhalb von `summarize()` müssen Sie `n = n()` schreiben, um die Anzahl der Messwerte zu erhalten.
1. Stellen Sie die Monatsmittel der Temperatur als Linien dar. Tipp: Um die Monate mit ihren Namen darzustellen, nutzen Sie den folgenden Code `scale_x_discrete(limits = as_factor(1:12), labels = month.abb)`. Hängen Sie ihn mit einem `+` an. Was macht dieser Code?
1. Beschriften Sie die Grafik sinnvoll.
1. Fügen Sie die Standardabweichungen als Band hinzu.

### Quantile
Wir beschäftigen uns mit dem Datensatz `possum` im Paket `openintro`.

1. Laden Sie die Biblothek und anschließend den Datensatz.

1. Berechnen Sie 

- Das 1. Quartil
- Das 3. Quartil
- Den Median

Der Körper- und Kopflängen.

2. Stellen Sie die Körper- und Kopflängen als Boxplots nebeneinander dar. Nutzen Sie dazu die Bibliothek `patchwork`.

3. Stellen Sie die beiden Variablen als Streudiagramm dar (Körperlängen auf die $x$-Achse).

4. Berechnen Sie den linearen Korrelationskoeffizienten mit der Funktion `cor()`.

## Umgang mit der Normalverteilung

### Simulieren von Daten aus einer Normalverteilung
1. Simulieren Sie 1000 Werte aus der Standardnormalverteilung. Nutzen Sie dazu die Funktion `rnorm()` und stellen Sie die Daten als Histogramm dar.  Tipp: Wandeln Sie die Daten in ein `tibble` um.

2. Die Funktion `dnorm` berechnet den Wert der Wahrscheinlichkeitsdichte $f(x)$, also einen Punkt auf der Glockenkurve. Berechnen Sie diesen Wert für $x = 0.3$ für die Standardnormalverteilung.

3. Die Funktion `dnorm` kann man dazu nutzen, um die theoretische Normalverteilung über die simulierten Daten aus Aufgabe 1 zu plotten. Nutzen Sie dazu die Funktionen `geom_density()` und `geom_function()`. Diese Aufgabe machen wir gemeinsam.

4. Überprüfen Sie, dass der Bereich $\pm$ 1.96 Standardabweichungen in einer Normalverteilung 95% der Werte enthält. Zeichnen Sie den Bereich richtig ein. 

## Mittelwert der Studiendauer in Werdeschlau
Die Studentenvertretung in Werdeschlau möchte wissen, wie hoch im Schnitt die Studiendauer an der Uni Werdeschlau beträgt.

Führen Sie eine Befragung von 100 zufällig ausgewählten Studierenden durch. Schätzen Sie aus diesen Daten die Studiendauer und geben Sie ein 95%-Konfidenzintervall an. Berechnen Sie dieses Konfidenzintervall

a. mit Bootstrap
b. mithilfe der Normalverteilung. Der Standardfehler des Mittelwerts sei 0.06 Jahre.

Vergleichen Sie die beiden Konfidenzintervalle.

## Umgang mit der $t$-Verteilung
1. Finden Sie den kritischen Wert $t^*_{2}$ für das 95%-Konfidenzintervall Nutzen Sie dazu die Funktion `qt()`.

2. Plotten Sie die dazugehörige Verteilung mit `normTail()` und markieren Sie den Bereich, der 95% aller Werte enthält.

3. Vergleichen Sie die mit dem kritischen Wert für eine $t$-Verteilung mit 18 Freiheitsgraden.

## Mitttelwert der Laufzeiten beim Cherrys Blossom Race
Beim Cherrys Blossom Race laufen die Teilnehmer ein 10-Meilen Rennen. Wie hoch ist die mittlere Laufzeit (mit 95%-Konfidenzintervall) im Jahr 2017? Der Datensatz `run17` enthält die Daten.

1. Filtern Sie zuerst nach `event == '10 Mile'`, da der Datensatz mehrere Rennen enthält (Hilfe lesen!).
2. Ziehen Sie eine Zufallsstichprobe von 100 Läufern und rechnen Sie die Laufzeit `net_sec` in Minuten um.
3. Überprüfen Sie die Anforderungen an die Daten. Welches Modell dürfen Sie nutzen?
4. Berechnen Sie die Punktschätzung und das Konfidenzintervall.


## Mitttelwert der Laufzeiten beim Cherrys Blossom Race -- mit `infer`

Lösen Sie die obige Aufgabe mit dem Paket `infer`.

## $t$-Test für den Mittelwert

Wird der typische US-Läufer mit der Zeit schneller oder langsamer? Wir betrachten diese Frage im Kontext des Cherrys Blossom Race, einem 10-Meilen-Lauf in Washington, DC, der jedes Frühjahr stattfindet. Die Durchschnittszeit aller Läufer, die den Kirschblütenlauf im Jahr 2016 beendeten, betrug 93.29 Minuten (93 Minuten und etwa 17 Sekunden). Anhand der Daten von 100 Teilnehmern des Kirschblütenlaufs 2017 möchten wir feststellen, ob die Läufer bei diesem Lauf schneller oder langsamer werden, oder ob es keine Veränderungen gibt.

Lösen Sie die Aufgabe mit `infer`.


### Bodenverdichtung {#verdichtung}
Schwere landwirtschaftliche Maschinen können beim Bearbeiten des Bodens zu Bodenverdichtung führen. In einem randomisierten Design wurden zufällig Parzellen auf einem sonst homogenen Feld mit einer schweren Maschine bearbeitet (`compacted`). Auf allen Parzellen wurde danach die Lagerungsdichte bestimmt. Aus langjährigen Messungen ist ist der Mittelwert des unverdichteten Bodens bekannt und beträgt 1.3 [g/cm³]. Die Lagerungsdichte (auch Trockenrohdichte) ist ein Maß für Bodenstruktur und gibt das Verhältnis der Trockenmasse eines Bodens zu seinem Volumen. Sie wird häufig in [g/cm³] gemessen und kann als ein Indikator für Bodenverdichtung genutzt werden. Eine Erhöhung der Lagerungsdichte ist ein Indikator für Verdichtung. Der Datensatz ist in der Datei "bd_compaction_simple.csv" gespeichert.

1. Überprüfen Sie, ob sich die Lagerungsdichte auf den bearbeiteten Feldern erhöht hat. 
